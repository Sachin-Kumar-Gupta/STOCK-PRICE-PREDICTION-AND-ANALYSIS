# -*- coding: utf-8 -*-
"""Stock Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bOjNF2_YS72WW6-w3l6RPs-wECpFNXxN
"""

#Install NLTK
!pip install nltk

#Install Gensim
# Gensim is a open source lib for unsupervised topic modeling and nlp
!pip install gensim

#Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#TensorFlow
import tensorflow as tf
from tensorflow.keras.preprocessing.text import one_hot,Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical

#NLP And Gensim
from wordcloud import WordCloud, STOPWORDS
import nltk
import re
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
import plotly.express as px

stock = pd.read_csv("stock_sentiment.csv")
stock

stock.info()

stock.isnull().sum()

stock["Sentiment"].value_counts()

sns.countplot(stock['Sentiment'])

"""#Data Cleaning"""

#Removing Puncuation From Text
import string
string.punctuation

def remove_pun(message):
  Test_punc_removed = [ char for char in message if char not in string.punctuation ]
  Test_join = ''.join(Test_punc_removed)
  return Test_join

stock['Text Without Punch'] = stock['Text'].apply(remove_pun)
stock

#Removing Stopwords
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use','will','aap','co','day','user','stock','today','week','year'])

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if len(token) >= 3 and token not in stop_words:
            result.append(token)
            
    return result

stock['Text Without Punc & Stopwords'] = stock['Text Without Punch'].apply(preprocess)
stock

"""#Plotting WordCloud"""

# join the words into a string
stock['Text Without Punc & Stopwords Joined'] = stock['Text Without Punc & Stopwords'].apply(lambda x: " ".join(x))
stock

#Most common words used in Positive Sentiment
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800).generate(" ".join(stock[stock['Sentiment'] == 1]['Text Without Punc & Stopwords Joined']))
plt.imshow(wc, interpolation = 'bilinear');

#Most common words used in Negative Sentiment
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800).generate(" ".join(stock[stock['Sentiment'] == 0]['Text Without Punc & Stopwords Joined']))
plt.imshow(wc, interpolation = 'bilinear');

"""#Visualizing Cleaned Dataset"""

nltk.download('punkt')

# word_tokenize is used to break up a string into words
print(stock['Text Without Punc & Stopwords Joined'][0])
print(nltk.word_tokenize(stock['Text Without Punc & Stopwords Joined'][0]))

# Obtain the maximum length of data in the document
# This will be later used when word embeddings are generated
maxlen = -1
for doc in stock['Text Without Punc & Stopwords Joined']:
    tokens = nltk.word_tokenize(doc)
    if(maxlen < len(tokens)):
        maxlen = len(tokens)
print("The maximum number of words in any document is:", maxlen)

tweets_length = [ len(nltk.word_tokenize(x)) for x in stock['Text Without Punc & Stopwords Joined'] ]
tweets_length

# Plot the distribution for the number of words in a text
fig = px.histogram(x = tweets_length, nbins = 50)
fig.show()

"""#PREPARE THE DATA BY TOKENIZING AND PADDING"""

# Obtain the total words present in the dataset
list_of_words = []
for i in stock['Text Without Punc & Stopwords']:
    for j in i:
        list_of_words.append(j)

list_of_words

# Obtain the total number of unique words
total_words = len(list(set(list_of_words)))
total_words

# split the data into test and train 
X = stock['Text Without Punc & Stopwords']
y = stock['Sentiment']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)

X_train.shape

X_test.shape

# Create a tokenizer to tokenize the words and create sequences of tokenized words
tokenizer = Tokenizer(num_words = total_words)
tokenizer.fit_on_texts(X_train)

# Training data
train_sequences = tokenizer.texts_to_sequences(X_train)

# Testing data
test_sequences = tokenizer.texts_to_sequences(X_test)

train_sequences

print("The encoding for document\n", X_train[1:2],"\n is: ", train_sequences[1])

# Add padding to training and testing
padded_train = pad_sequences(train_sequences, maxlen = 29)
padded_test = pad_sequences(test_sequences, maxlen = 29)

for i, doc in enumerate(padded_train[:3]):
     print("The padded encoding for document:", i+1," is:", doc)

# Convert the data to categorical 2D representation
y_train_cat = to_categorical(y_train, 2)
y_test_cat = to_categorical(y_test, 2)

y_train_cat.shape

"""#Training Model"""

# Sequential Model
model = Sequential()

# embedding layer
model.add(Embedding(total_words, output_dim = 512))

# Bi-Directional RNN and LSTM
model.add(LSTM(256))

# Dense layers
model.add(Dense(128, activation = 'relu'))
model.add(Dropout(0.3))
model.add(Dense(2,activation = 'softmax'))
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])
model.summary()

# train the model
model.fit(padded_train, y_train_cat, batch_size = 32, validation_split = 0.2, epochs = 2)

# make prediction
pred = model.predict(padded_test)

# make prediction
prediction = []
for i in pred:
  prediction.append(np.argmax(i))

# list containing original values
original = []
for i in y_test_cat:
  original.append(np.argmax(i))

# acuracy score on text data
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(original, prediction)
accuracy

# Plot the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(original, prediction)
sns.heatmap(cm, annot = True)